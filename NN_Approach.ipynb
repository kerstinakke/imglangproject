{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/clean.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "for i,sample in enumerate(data[:4]):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    img = mpimg.imread(\"data/lfw/\"+sample[\"image\"].replace('bmp',\"jpg\"))\n",
    "    plt.imshow(img)\n",
    "    plt.text(300,50,\"\\n\".join([desc['text'] for desc in sample['descriptions']]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import transparent_latent_gan.src.model.cnn_face_attr_celeba as cnn_face\n",
    "from transparent_latent_gan.src.tl_gan import feature_axis\n",
    "import glob, os,pickle\n",
    "import transparent_latent_gan.src.tl_gan.feature_celeba_organize as feature_celeba_organize\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_face.create_cnn_model(path_celeba_att=\"transparent_latent_gan/data/raw/celebA_annotation/list_attr_celeba.txt\")\n",
    "model.load_weights(cnn_face.get_list_model_save(path_model_save = 'transparent_latent_gan/asset_model/cnn_face_attr_celeba')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feature_direction = './transparent_latent_gan/asset_results/pg_gan_celeba_feature_direction_40'\n",
    "\n",
    "pathfile_feature_direction = glob.glob(os.path.join(path_feature_direction, 'feature_direction_*.pkl'))[-1]\n",
    "\n",
    "with open(pathfile_feature_direction, 'rb') as f:\n",
    "    feature_direction_name = pickle.load(f)\n",
    "\n",
    "feature_direction = feature_direction_name['direction']\n",
    "feature_name = feature_direction_name['name']\n",
    "num_feature = feature_direction.shape[1]\n",
    "\n",
    "\n",
    "importlib.reload(feature_celeba_organize)\n",
    "feature_name = feature_celeba_organize.feature_name_celeba_rename\n",
    "feature_direction = feature_direction_name['direction'] * feature_celeba_organize.feature_reverse[None, :]\n",
    "\n",
    "len_z = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features, single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_img_batch = []\n",
    "img = PIL.Image.open(\"data/lfw/\"+data[215][\"image\"].replace('bmp',\"jpg\")).resize((128,128),PIL.Image.BICUBIC)\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "list_img_batch.append(img)\n",
    "\n",
    "img_batch = np.stack(list_img_batch, axis=0)\n",
    "x = cnn_face.preprocess_input(img_batch)\n",
    "y = model.predict(x, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(y[0], feature_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modification to get more diverse results\n",
    "dont_change =  ['Male','Hairline','Big_lips','Big_nose','Narrow_Eyes','Skin_Tone','Smiling','Mouth_Open','Age']\n",
    "np.array([(x if x > 0 or feature_name[i] in dont_change else 0) for i,x in enumerate(y[0])])\n",
    "# uncomment next to see unmodified \n",
    "# y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_img_batch = []\n",
    "for d in data:   \n",
    "    img = np.asarray(PIL.Image.open(\"data/lfw/\"+d[\"image\"].replace('bmp',\"jpg\")).resize((128,128),PIL.Image.BICUBIC)) # uncropped\n",
    "    # PIL.Image.open(\"data/lfw/\"+data[215][\"image\"].replace('bmp',\"jpg\")).crop((125-64,125-64,125+64,125+64)) # cropped\n",
    "    list_img_batch.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = np.stack(list_img_batch, axis=0)\n",
    "x = cnn_face.preprocess_input(img_batch)\n",
    "y = model.predict(x, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loc = \"NN_Approach_results/plain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modification for more diverse results\n",
    "new_y =[] \n",
    "for vec in y:\n",
    "    v = [(x if x > 0 or feature_name[i] in dont_change else 0) for i,x in enumerate(vec)]\n",
    "    new_y.append(v)\n",
    "    np.savetxt(target_loc + \"image_features_modified.txt\",np.array(new_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(target_loc+ \"image_features_modified.txt\",np.array(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output files can be used in kaggle kernel that runs tl-GAN demo to predict images for each vector. The necessary code is in cell below.\n",
    "Instructions for running tl-gan demo [https://github.com/SummitKwan/transparent_latent_gan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### copy this cell to tl-GAN demo krnel to get output\n",
    "!mkdir \"/kaggle/working/generated_faces\"\n",
    "feat_vecs = np.loadtxt(\"../../../simple-feature-extraction-results/simple_feats_v4.txt\") # change to location of your dataset\n",
    "for i, feature_vec in enumerate(feat_vecs[:10]):\n",
    "    z_sample = feature_direction.dot(feature_vec) #np.random.randn(len_z)\n",
    "    # the generated image using this noise patter\n",
    "    x_sample = generate_image.gen_single_img(z=z_sample, Gs=Gs)\n",
    "    imgObj = PIL.Image.fromarray(x_sample)\n",
    "    imgObj.save(\"/kaggle/working/generated_faces/test_{}.png\".format(i), format='PNG')\n",
    "shutil.make_archive(\"/kaggle/working/generated_faces\", 'zip', \"/kaggle/working/generated_faces\")\n",
    "!rm -r /kaggle/working/generated_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data and training the model\n",
    "**Won't give reasonable results because fearure extraction fails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features = np.loadtxt(\"image_features_modified.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['descriptions'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data\n",
    "\n",
    "embed = load_vectors(\"wiki-news-300d-1M-subword.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_to_img = []\n",
    "sentences = []\n",
    "for i,sample in enumerate(data):\n",
    "    \"\"\"\"\" separate descriptions \n",
    "    for desc in sample['descriptions']:\n",
    "        text = word_tokenize(desc['text'].lower()) \n",
    "        sent_to_img.append(i)\n",
    "        sentences.append(text)\n",
    "    \"\"\"\"\" \n",
    "    text = word_tokenize(\" \".join([desc['text'].lower() for desc in sample['descriptions']]))\n",
    "    sent_to_img.append(i)\n",
    "    sentences.append(text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([s for sent in sentences for s in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((len(vocab)+1, 300))\n",
    "for i, word in enumerate([\"<pad>\"]+vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = embed[word]\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.rand(300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "for i, word in enumerate([\"<pad>\"]+vocab):\n",
    "    index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[index[w] for w in txt if w in index] for txt in sentences])\n",
    "Y = np.array([out_features[i] for i in sent_to_img])\n",
    "val_idx = np.random.randint(0, X.shape[0], 20)\n",
    "train_idx = np.full(X.shape[0],True)\n",
    "train_idx[val_idx]=False\n",
    "X_train = X[train_idx]\n",
    "Y_train = Y[train_idx]\n",
    "X_val = X[val_idx]\n",
    "Y_val = Y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import ArrayField\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from typing import Iterator, List, Dict\n",
    "\n",
    "class InstanceMaker(DatasetReader):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "\n",
    "    def vec_to_instance(self, description: List[int], features: List[float] = np.zeros(1)) -> Instance:\n",
    "        desc_field =  ArrayField(np.array(description,int))\n",
    "        fields = {\"description\": desc_field, \"num_tokens\": len(description)}\n",
    "\n",
    "        if features.any():\n",
    "            features_field = ArrayField(np.array(features))\n",
    "            fields[\"features\"] = features_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, encoded_set) -> Iterator[Instance]:\n",
    "        for desc, feat in encoded_set:\n",
    "            yield self.vec_to_instance(desc,feat)\n",
    "            \n",
    "            \n",
    "\n",
    "class TextToFeature(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, out_size, weights_matrix):\n",
    "        super(TextToFeature, self).__init__()        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(weights_matrix.shape[1], hidden_size)\n",
    "        self.vec2out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        embedded = self.embedding(x)\n",
    "        o, hidden = self.lstm(embedded.view(embedded.size()[1], embedded.size()[0], -1))\n",
    "        hidden = hidden[0].view(hidden[0].size()[1], -1)\n",
    "        out = self.vec2out(hidden)\n",
    "        return torch.tanh(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker = InstanceMaker()\n",
    "train_set = maker.read(zip(X_train,Y_train))\n",
    "val_set = maker.read(zip(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.sort(key = lambda x: x['num_tokens']) # to group with similar length\n",
    "train_set.reverse()\n",
    "val_set.sort(key = lambda x: x['num_tokens']) # to group with similar length\n",
    "val_set.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "hidden_size = 800\n",
    "\n",
    "model = TextToFeature(hidden_size, 40, weights_matrix)\n",
    "criterion = torch.nn.functional.hinge_embedding_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "num_instances = len(train_set)\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "best_v = 100\n",
    "\n",
    "for i in range(epochs): \n",
    "    losses = 0\n",
    "    model.train()\n",
    "    for j in range(0,num_instances,batch_size):       \n",
    "        batch = train_set[j:j+batch_size]\n",
    "        pad_len=batch[0]['num_tokens'] # pad based on longst in batch\n",
    "        np.random.shuffle(batch) \n",
    "        \n",
    "        X = torch.stack([element['description'].as_tensor({'dimension_0':pad_len}).long() for element in batch])\n",
    "        Y = torch.stack([element['features'].as_tensor({'dimension_0':40}) for element in batch])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(X) \n",
    "        loss = criterion((Y-out).abs(), torch.tensor(np.ones(Y.shape)) )\n",
    "        losses += loss.item()\n",
    "        loss.backward()                                   \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    pad_len = val_set[0]['num_tokens']\n",
    "    X = torch.stack([element['description'].as_tensor({'dimension_0':pad_len}).long() for element in val_set])\n",
    "    Y = torch.stack([element['features'].as_tensor({'dimension_0':40}) for element in val_set])\n",
    "    \n",
    "    out = model.forward(X)      \n",
    "    loss = criterion((Y - out).abs(), torch.tensor(np.ones(Y.shape)))\n",
    "    if loss.item() < best_v:\n",
    "        best_v = loss.item()\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    tr_losses.append(losses/np.ceil(num_instances/batch_size))\n",
    "    val_losses.append(loss.item())                                    \n",
    "    print(\"eval loss:\",loss)\n",
    "    print(\"avg training loss:\",tr_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(tr_losses)),tr_losses,label=\"train\")\n",
    "plt.plot(np.arange(len(tr_losses)),val_losses,label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextToFeature(hidden_size, 40, weights_matrix)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "pad_len = val_set[0]['num_tokens']\n",
    "X = torch.stack([element['description'].as_tensor({'dimension_0':pad_len}).long() for element in val_set])\n",
    "out = model.forward(X) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_lang",
   "language": "python",
   "name": "img_lang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
